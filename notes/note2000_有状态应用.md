#  一个 MySQL 集群

首先，用自然语言来描述一下我们想要部署的“有状态应用”。

是一个“主从复制”（Maser-Slave Replication）的 MySQL 集群；

有 1 个主节点（Master）；

有多个从节点（Slave）；

从节点需要能水平扩展；

所有的写操作，只能在主节点上执行；

读操作可以在所有节点上执行。

# 传统

在安装好 MySQL 的 Master 节点之后，你需要做的第一步工作，就是通过 XtraBackup 将 Master 节点的数据备份到指定目录。

第二步：配置 Slave 节点。Slave 节点在第一次启动前，需要先把 Master 节点的备份数据，连同备份信息文件，一起拷贝到自己的数据目录（/var/lib/mysql）下  
```
TheSlave|mysql> CHANGE MASTER TO
                MASTER_HOST='$masterip',
                MASTER_USER='xxx',
                MASTER_PASSWORD='xxx',
                MASTER_LOG_FILE='TheMaster-bin.000001',
                MASTER_LOG_POS=481;
```
第三步，启动 Slave 节点。
它会使用备份信息文件中的二进制日志文件和偏移量，与主节点进行数据同步。

第四步，在这个集群中添加更多的 Slave 节点。  
需要注意的是，新添加的 Slave 节点的备份数据，来自于已经存在的 Slave 节点。


将部署 MySQL 集群的流程迁移到 Kubernetes 项目上，需要能够“容器化”地解决下面的“三座大山”：

    Master 节点和 Slave 节点需要有不同的配置文件（即：不同的 my.cnf）；

    Master 节点和 Salve 节点需要能够传输备份信息文件；

    在 Slave 节点第一次启动之前，需要执行一些初始化 SQL 操作；

MySQL 本身同时拥有拓扑状态（主从节点的区别）和存储状态（MySQL 保存在本地的数据），我们自然要通过 StatefulSet 来解决这“三座大山”的问题。

## “第一座大山：Master 节点和 Slave 节点需要有不同的配置文件”，

    很容易处理：我们只需要给主从节点分别准备两份不同的 MySQL 配置文件，然后根据 Pod 的序号（Index）挂载进去即可。

    接下来，我们需要创建两个 Service 来供 StatefulSet 以及用户使用

## 第二座大山：Master 节点和 Salve 节点需要能够传输备份文件  
翻越这座大山的思路，我比较推荐的做法是：先搭建框架，再完善细节。其中，Pod 部分如何定义，是完善细节时的重点。

首先，我们先为 StatefulSet 对象规划一个大致的框架，如下图所示：

![](img/2001.png)

Headless Service 的名字是：mysql。  
StatefulSet 的 replicas 值是 3，表示它定义的 MySQL 集群有三个节点  


设计一下这个 StatefulSet 的 Pod 模板，也就是 template 字段

    如果这个 Pod 是 Master 节点，我们要怎么做；

    如果这个 Pod 是 Slave 节点，我们又要怎么做。

第一步：从 ConfigMap 中，获取 MySQL 的 Pod 对应的配置文件。

第二步：在 Slave Pod 启动前，从 Master 或者其他 Slave Pod 里拷贝数据库数据到自己的目录下。



##  一个 Slave 角色的 MySQL 容器启动之前，谁能负责给它执行初始化的 SQL 语句呢？  
“第三座大山”的问题，即：如何在 Slave 节点的 MySQL 容器第一次启动之前，执行初始化 SQL。  

在这个名叫 xtrabackup 的 sidecar 容器的启动命令里，其实实现了两部分工作。

第一部分工作，当然是 MySQL 节点的初始化工作。

在完成 MySQL 节点的初始化后，这个 sidecar 容器的第二个工作，则是启动一个数据传输服务。



## 至此，一个完整的主从复制模式的 MySQL 集群就定义完了。

可以使用 kubectl 命令，尝试运行一下这个 StatefulSet 了。

然后，我们就可以创建这个 StatefulSet 了

接下来，我们可以尝试向这个 MySQL 集群发起请求，执行一些 SQL 操作来验证它是否正常：


# 总结
